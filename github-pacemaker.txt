Application load is essentially a network partition:

"Monday's migration caused higher load on the database than our operations team
has previously seen during these sorts of migrations. So high, in fact, that
they caused Percona Replication Manager's health checks to fail on the master.
In response to the failed master health check, Percona Replication manager
moved the 'active' role and the master database to another server in the
cluster and stopped MySQL on the node it perceived as failed.

At the time of this failover, the new database selected for the 'active' role
had a cold InnoDB buffer pool and performed rather poorly. The system load
generated by the site's query load on a cold cache soon caused Percona
Replication Manager's health checks to fail again, and the 'active' role failed
back to the server it was on originally."


"Upon attempting to disable maintenance-mode, a Pacemaker segfault occurred
that resulted in a cluster state partition. After this update, two nodes (I'll
call them 'a' and 'b') rejected most messages from the third node ('c'), while
the third node rejected most messages from the other two. Despite having
configured the cluster to require a majority of machines to agree on the state
of the cluster before taking action, two simultaneous master election decisions
were attempted without proper coordination. In the first cluster, master
election was interrupted by messages from the second cluster and MySQL was
stopped.

In the second, single-node cluster, node 'c' was elected at 8:19 AM, and any
subsequent messages from the other two-node cluster were discarded. As luck
would have it, the 'c' node was the node that our operations team previously
determined to be out of date. We detected this fact and powered off this
out-of-date node at 8:26 AM to end the partition and prevent further data
drift, taking down all production database access and thus all access to
github.com."

"In situations where the id MySQL generated for a record is used to query data
in Redis, the cross-data-store foreign key relationships became out of sync for
records created during this window.

"Consequentially, some events created during this window appeared on the wrong
users' dashboards. Also, some repositories created during this window were
incorrectly routed... 16 of these repositories were private, and for seven
minutes from 8:19 AM to 8:26 AM PDT on Tuesday, Sept 11th, were accessible to
people outside of the repository's list of collaborators or team members."

"In each situation in which that occurred, if any member of our operations team
had been asked if the failover should have been performed, the answer would
have been a resounding no."

jnewland: "Github availabiltiy this week"
https://github.com/blog/1261-github-availability-this-week
